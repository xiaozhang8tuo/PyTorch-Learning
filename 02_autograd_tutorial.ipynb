{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd: Automatic Differentiation（自动微分）\n",
    "===================================\n",
    "\n",
    "PyTorch中所有神经网络的核心是 ``autograd`` 包.\n",
    "让我们先简单地看一下这个，然后我们来训练我们的第一个神经网络。\n",
    "\n",
    "\n",
    "The ``autograd`` 为张量上的所有操作提供自动微分. \n",
    "它是一个**按照代码运行**的框架，这意味着你的反向传播是由您的代码运行方式定义，并且每次迭代都可以不同。\n",
    "\n",
    "让我们用更简单的术语和一些例子来看看。\n",
    "\n",
    "Tensor\n",
    "--------\n",
    "\n",
    "``torch.Tensor`` 是``autograd`` 包中的主要类. 将属性``.requires_grad`` 设置为 ``True``, 她将回按照你定义的方式进行. 当你完成你的代码后，可以调用 ``.backward()`` 自动的计算所有的梯度. 这个张量的梯度将会被积累到 ``.grad`` 属性中.\n",
    "\n",
    "要阻止一个张量跟踪历史, 调用 ``.detach()`` 把它从计算历史中分离出来, 并防止未来的计算被跟踪，防止跟踪历史 (占用内存), 您还可以将代码块封装进来 ``with torch.no_grad():``. 这在评估模型时特别有用，因为模型训练得到的参数是我们感兴趣的\n",
    "``requires_grad=True``, 但是我们并不需要梯度.\n",
    "\n",
    "还有一个类对autograd实现非常重要的 ``Function``.\n",
    "\n",
    "``Tensor`` 和 ``Function`` 是相互联系的，它们构成一个无环图, 图中编码着完整的计算历史. 每个张量\n",
    "的 ``.grad_fn`` 属性关联着一个创造该张量的``Function``（8是由4和4相加得到的）(用户直接创建的张量除外（石头里蹦出来的）- 他们的\n",
    "``grad_fn is None``).\n",
    "\n",
    "如果你想计算导数, 调用 ``.backward()`` 对一个 ``Tensor``. 如果 ``Tensor`` 时一个单元素张量 (i.e. 它包含一个单元素数据), 不需要具体化任何参数 ``backward()``,\n",
    "但是如果它有更多的元素, 你需要具体化 ``gradient``\n",
    "参数（设置梯度匹配的形状）.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个张量并设置 ``requires_grad=True`` 用它来跟踪计算\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做一个张量运算:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` 是加创建的结果, 所以他应该有一个 ``grad_fn``属性.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x0000020EBC9D4780>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do more operations on ``y``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_( ... )`` 改变现有张量的 ``requires_grad``标志位\n",
    ". 输入``requires_grad``标志位默认为 ``False`` .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000020EAC64A828>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients\n",
    "---------\n",
    "开始反向传播<br>\n",
    "因为 ``out`` 包含单个标量, ``out.backward()`` \n",
    "相当于 ``out.backward(torch.tensor(1.))``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients d(out)/dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个矩阵 ``4.5``. 把 ``out``写成\n",
    "*Tensor* “$o$”.\n",
    "o是由f（x）得到的 $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "然后,反向传播对o求x的导\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, 因此\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, if you have a vector valued function $\\vec{y}=f(\\vec{x})$,\n",
    "then the gradient of $\\vec{y}$ with respect to $\\vec{x}$\n",
    "is a Jacobian matrix:\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "Generally speaking, ``torch.autograd`` is an engine for computing\n",
    "vector-Jacobian product. That is, given any vector\n",
    "$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$,\n",
    "compute the product $v^{T}\\cdot J$. If $v$ happens to be\n",
    "the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$,\n",
    "that is,\n",
    "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$,\n",
    "then by the chain rule, the vector-Jacobian product would be the\n",
    "gradient of $l$ with respect to $\\vec{x}$:\n",
    "\n",
    "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "(Note that $v^{T}\\cdot J$ gives a row vector which can be\n",
    "treated as a column vector by taking $J^{T}\\cdot v$.)\n",
    "\n",
    "This characteristic of vector-Jacobian product makes it very\n",
    "convenient to feed external gradients into a model that has\n",
    "non-scalar output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PT中求导只能是标对矢量<br>\n",
    "标量（单个元素好比LOSS）对矢量（每一层的参数）求导-----导数：向量形式（可能是多个雅可比乘矢量的形式）<br>\n",
    "矢量对矢量求导-----导数：雅可比矩阵形式（PT中不能显示）<br>\n",
    "**--[Pytorch中的backward]**(https://blog.csdn.net/witnessai1/article/details/79763596)<br>\n",
    "熟悉内部运行机制同样可以得到雅可比矩阵，backward（）传入偏导的系数即v\\[1,0,0\\]得到第一列，在\\[0,1,0\\]类推"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at an example of vector-Jacobian product:<br>\n",
    "--[Pytorch中的backward](https://blog.csdn.net/witnessai1/article/details/79763596)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1482, -0.8346, -1.1552], requires_grad=True)\n",
      "tensor([ 1175.7565,  -854.6229, -1182.9468], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this case ``y`` is no longer a scalar. ``torch.autograd``\n",
    "could not compute the full Jacobian directly, but if we just\n",
    "want the vector-Jacobian product, simply pass the vector to\n",
    "``backward`` as argument:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors\n",
    "with ``.requires_grad=True`` by wrapping the code block in\n",
    "``with torch.no_grad():``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Later:**\n",
    "\n",
    "Documentation of ``autograd`` and ``Function`` is at\n",
    "https://pytorch.org/docs/autograd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
